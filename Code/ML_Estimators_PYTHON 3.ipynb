{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26eb87a",
   "metadata": {},
   "source": [
    "## Core Functions\n",
    "These handle importing necessary libraries, preparation of the feature arrays for Machine Learning, and execution of Machine Learning training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f008279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.26/04\n",
      "\n",
      "Ready! 2022/12/11 23:38:58\n"
     ]
    }
   ],
   "source": [
    "from ML_Python.ML_Python_Build_FeatureArrays_FromROOT import Build_FeatureArrays_FromROOT\n",
    "from ML_Python.ML_Python_TrainTest import (\n",
    "    Build_FeatureArrays_FromCSV,\n",
    "    Write_MLResults_ToCSV,\n",
    "    Write_MLWeights_ToCSV,\n",
    "    Train_All_Estimators,\n",
    "    Train_LinearRegression,\n",
    "    Train_RandomForestRegression,\n",
    "    Train_MLPRegression,\n",
    "    Test_Estimator,\n",
    "    Test_All_Estimators)\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def Build_SelectFeatureArray(\n",
    "    X_features,\n",
    "    feature_index\n",
    "    ) :\n",
    "    \"\"\"\n",
    "    Builds training and testing data sets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Selecting data from master array...\")\n",
    "    \n",
    "    X_features_select = []\n",
    "    for i in range(len(X_features)):\n",
    "        X_temp = []\n",
    "        for j in range(len(feature_index)):\n",
    "            X_temp.append(X_features[i][feature_index[j]])\n",
    "        X_features_select.append(X_temp)\n",
    "        \n",
    "    print(\"Data ready. Feature array length:\", len(X_features_select), \"\\n\")\n",
    "    \n",
    "    return X_features_select\n",
    "\n",
    "    \n",
    "\n",
    "def TestAndSave_LinearRegression(\n",
    "    feature_label,    # Array of labels corresponding to each feature\n",
    "    feature_index,    # Array of indices for each feature used in X_train\n",
    "    lr_pipeline,      # Trained Linear Regression Pipeline\n",
    "    lr_coeffs,        # Array of coefficient values from trained linear regression pipeline\n",
    "    X_test_select,    # Array of testing data features\n",
    "    y_test,           # Array of testing data targets\n",
    "    sc_test,          # Array of testing data simple corrections\n",
    "    pt_test_min,      # Float of min pT to test with\n",
    "    pt_test_max,      # Float of max pT to test with\n",
    "    output_filename,  # Directory path + name for output csv file\n",
    "    use_scaler = True # If true, rescales data\n",
    "    ) :\n",
    "    \n",
    "    X_test_temp  = []\n",
    "    y_test_temp  = []\n",
    "    sc_test_temp = []\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] > pt_test_min and y_test[i] < pt_test_max:\n",
    "            X_test_temp.append(X_test_select[i])\n",
    "            y_test_temp.append(y_test[i])\n",
    "            sc_test_temp.append(sc_test[i])\n",
    "        else: continue\n",
    "    \n",
    "    # Tests estimator\n",
    "    \n",
    "    print(type(lr_pipeline))\n",
    "    \n",
    "    lr_results, lr_results_delta = Test_Estimator(\n",
    "        lr_pipeline,\n",
    "        X_test_temp, \n",
    "        y_test_temp\n",
    "        )\n",
    "    \n",
    "    # Writes outputs to a csv file\n",
    "    Write_MLResults_ToCSV(\n",
    "        output_filename,\n",
    "        y_test_temp,\n",
    "        sc_test_temp,\n",
    "        lr_results,\n",
    "        X_test_temp,\n",
    "        feature_label\n",
    "        )\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def TrainTestPlot_All_Estimators(\n",
    "    feature_label,    # Array of labels corresponding to each feature\n",
    "    feature_index,    # Array of indices for each feature used in X_train\n",
    "    X_train,          # Array of training data features\n",
    "    y_train,          # Array of training data targets\n",
    "    sc_train,         # Array of training data simple correction values\n",
    "    X_test,           # Array of testing data features\n",
    "    y_test,           # Array of testing data targets\n",
    "    sc_test,          # Array of testing data simple corrections\n",
    "    output_file_path, # File path for outputs\n",
    "    use_scaler = True,\n",
    "    use_lr = True,\n",
    "    use_rf = True,\n",
    "    use_mlp = True,\n",
    "    ) :\n",
    "    \n",
    "    # Builds training data set\n",
    "    print(\"Selecting training data...\")\n",
    "    X_train_select = []\n",
    "    for i in range(len(X_train)):\n",
    "        X_temp = []\n",
    "        for j in range(len(feature_index)):\n",
    "            X_temp.append(X_train[i][feature_index[j]])\n",
    "        X_train_select.append(X_temp)\n",
    "    print(\"Training data ready. X/Y length:\", len(X_train_select), len(y_train), \"/n\")\n",
    "    \n",
    "    # Builds pipelines from selected training features\n",
    "    print(\"Building estimator pipelines...\")\n",
    "    lr_pipeline, rf_pipeline, mlp_pipeline, lr_coeffs, rf_features = Train_All_Estimators(\n",
    "        X_train_select, y_train, feature_label, \n",
    "        use_StandardScaler = use_scaler,\n",
    "        use_LinearRegression = use_lr,\n",
    "        use_RandomForest = use_rf,\n",
    "        use_MLP = use_mlp)\n",
    "    print(\"Pipelines built./n\")\n",
    "    \n",
    "    print(\"Selecting testing data...\")\n",
    "    X_test_select = []\n",
    "    for i in range(len(X_test)):\n",
    "        X_temp = []\n",
    "        for j in range(len(feature_index)):\n",
    "            X_temp.append(X_test[i][feature_index[j]])\n",
    "        X_test_select.append(X_temp)\n",
    "    print(\"Testing data ready. X/Y length:\", len(X_test_select), len(y_test), \"/n\")\n",
    "    \n",
    "    # Test estimators\n",
    "    print(\"Testing all estimators...\")\n",
    "    lr_results, lr_results_delta, rf_results, rf_results_delta, mlp_results, mlp_results_delta = Test_All_Estimators(\n",
    "        X_test_select, \n",
    "        y_test, \n",
    "        lr_pipeline,\n",
    "        rf_pipeline,\n",
    "        mlp_pipeline)\n",
    "    print(\"Estimator testing complete!/n\")\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print(\"\\nReady!\", dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495115a",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Sets up input directories/files for training and testing. Creates initial training data arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61026866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to collect data from csv backup file...\n",
      "Jet: 10000 | pTraw: 69.750 | pTcorr:  19.722 | pTtrue:  21.433\n",
      "Jet: 20000 | pTraw: 87.747 | pTcorr:  24.601 | pTtrue:  27.341\n",
      "Jet: 30000 | pTraw: 47.990 | pTcorr:  3.316 | pTtrue:  13.372\n",
      "Jet: 40000 | pTraw: 73.657 | pTcorr:  17.213 | pTtrue:  10.965\n",
      "Jet: 50000 | pTraw: 155.385 | pTcorr:  84.131 | pTtrue:  84.254\n",
      "Jet: 60000 | pTraw: 67.146 | pTcorr:  25.140 | pTtrue:  19.837\n",
      "Jet: 70000 | pTraw: 36.326 | pTcorr:  14.513 | pTtrue:  15.810\n",
      "Jet: 80000 | pTraw: 118.382 | pTcorr:  62.321 | pTtrue:  62.664\n",
      "Jet: 90000 | pTraw: 118.381 | pTcorr:  77.046 | pTtrue:  84.791\n",
      "Jet: 100000 | pTraw: 142.244 | pTcorr:  85.009 | pTtrue:  86.439\n",
      "Jet: 110000 | pTraw: 105.900 | pTcorr:  35.717 | pTtrue:  54.590\n",
      "Jet: 120000 | pTraw: 69.499 | pTcorr:  26.358 | pTtrue:  21.755\n",
      "Jet: 130000 | pTraw: 87.513 | pTcorr:  40.364 | pTtrue:  27.490\n",
      "Jet: 140000 | pTraw: 76.336 | pTcorr:  15.606 | pTtrue:  10.107\n",
      "Jet: 150000 | pTraw: 12.131 | pTcorr:  6.389 | pTtrue:  10.620\n",
      "Jet: 160000 | pTraw: 70.809 | pTcorr:  12.266 | pTtrue:  14.921\n",
      "Jet: 170000 | pTraw: 53.846 | pTcorr:  15.480 | pTtrue:  14.306\n",
      "Jet: 180000 | pTraw: 155.480 | pTcorr:  87.655 | pTtrue:  88.639\n",
      "Jet: 190000 | pTraw: 103.234 | pTcorr:  57.354 | pTtrue:  55.110\n",
      "Jet: 200000 | pTraw: 82.518 | pTcorr:  18.205 | pTtrue:  20.951\n",
      "Jet: 210000 | pTraw: 98.974 | pTcorr:  46.053 | pTtrue:  34.194\n",
      "Jet: 220000 | pTraw: 22.880 | pTcorr:  2.579 | pTtrue:  15.306\n",
      "Jet: 230000 | pTraw: 89.575 | pTcorr:  29.065 | pTtrue:  25.876\n",
      "Jet: 240000 | pTraw: 63.619 | pTcorr:  17.794 | pTtrue:  17.812\n",
      "Jet: 250000 | pTraw: 86.673 | pTcorr:  32.012 | pTtrue:  30.974\n",
      "Jet: 260000 | pTraw: 67.952 | pTcorr:  17.573 | pTtrue:  13.033\n",
      "Jet: 270000 | pTraw: 59.419 | pTcorr:  23.128 | pTtrue:  18.864\n",
      "Jet: 280000 | pTraw: 98.592 | pTcorr:  48.819 | pTtrue:  55.962\n",
      "Jet: 290000 | pTraw: 130.747 | pTcorr:  77.630 | pTtrue:  82.714\n",
      "Jet: 300000 | pTraw: 120.269 | pTcorr:  67.950 | pTtrue:  71.927\n",
      "Jet: 310000 | pTraw: 74.777 | pTcorr:  18.914 | pTtrue:  17.240\n",
      "Jet: 320000 | pTraw: 52.751 | pTcorr:  3.441 | pTtrue:  23.480\n",
      "Jet: 330000 | pTraw: 103.428 | pTcorr:  54.766 | pTtrue:  39.969\n",
      "Jet: 340000 | pTraw: 98.993 | pTcorr:  36.874 | pTtrue:  39.750\n",
      "Jet: 350000 | pTraw: 79.689 | pTcorr:  18.207 | pTtrue:  16.660\n",
      "Jet: 360000 | pTraw: 56.471 | pTcorr:  7.462 | pTtrue:  14.785\n",
      "Jet: 370000 | pTraw: 126.746 | pTcorr:  65.038 | pTtrue:  62.265\n",
      "Jet: 380000 | pTraw: 75.069 | pTcorr:  31.732 | pTtrue:  25.617\n",
      "Jet: 390000 | pTraw: 86.774 | pTcorr:  29.834 | pTtrue:  26.393\n",
      "Jet: 400000 | pTraw: 118.690 | pTcorr:  68.049 | pTtrue:  69.397\n",
      "Jet: 410000 | pTraw: 86.131 | pTcorr:  34.444 | pTtrue:  42.996\n",
      "Jet: 420000 | pTraw: 141.539 | pTcorr:  85.801 | pTtrue:  85.859\n",
      "Jet: 430000 | pTraw: 79.726 | pTcorr:  31.824 | pTtrue:  21.931\n",
      "Jet: 440000 | pTraw: 69.487 | pTcorr:  14.233 | pTtrue:  17.346\n",
      "Jet: 450000 | pTraw: 82.193 | pTcorr:  34.241 | pTtrue:  31.151\n",
      "Jet: 460000 | pTraw: 83.632 | pTcorr:  20.831 | pTtrue:  17.825\n",
      "Jet: 470000 | pTraw: 77.471 | pTcorr:  21.609 | pTtrue:  31.584\n",
      "Jet: 480000 | pTraw: 119.581 | pTcorr:  70.233 | pTtrue:  56.863\n",
      "Jet: 490000 | pTraw: 111.294 | pTcorr:  52.920 | pTtrue:  47.666\n",
      "Jet: 500000 | pTraw: 25.792 | pTcorr:  8.965 | pTtrue:  40.189\n",
      "Jet: 510000 | pTraw: 118.627 | pTcorr:  51.123 | pTtrue:  72.706\n",
      "Jet: 520000 | pTraw: 81.881 | pTcorr:  43.260 | pTtrue:  49.134\n",
      "Jet: 530000 | pTraw: 117.830 | pTcorr:  76.733 | pTtrue:  78.806\n",
      "Jet: 540000 | pTraw: 82.985 | pTcorr:  19.819 | pTtrue:  21.605\n",
      "Jet: 550000 | pTraw: 104.344 | pTcorr:  56.123 | pTtrue:  58.942\n",
      "Jet: 560000 | pTraw: 129.758 | pTcorr:  87.323 | pTtrue:  83.770\n",
      "Jet: 570000 | pTraw: 48.942 | pTcorr:  6.603 | pTtrue:  14.047\n",
      "Jet: 580000 | pTraw: 7.056 | pTcorr:  1.627 | pTtrue:  61.597\n",
      "Jet: 590000 | pTraw: 90.405 | pTcorr:  41.110 | pTtrue:  40.440\n",
      "Jet: 600000 | pTraw: 93.196 | pTcorr:  65.380 | pTtrue:  63.540\n",
      "Jet: 610000 | pTraw: 95.974 | pTcorr:  30.397 | pTtrue:  14.817\n",
      "Jet: 620000 | pTraw: 5.283 | pTcorr:  2.096 | pTtrue:  47.157\n",
      "Jet: 630000 | pTraw: 63.871 | pTcorr:  10.454 | pTtrue:  15.235\n",
      "Jet: 640000 | pTraw: 66.387 | pTcorr:  15.670 | pTtrue:  15.954\n",
      "Jet: 650000 | pTraw: 129.302 | pTcorr:  78.557 | pTtrue:  60.284\n",
      "Jet: 660000 | pTraw: 98.883 | pTcorr:  49.515 | pTtrue:  46.579\n",
      "Jet: 670000 | pTraw: 133.212 | pTcorr:  85.872 | pTtrue:  71.251\n",
      "Jet: 680000 | pTraw: 107.277 | pTcorr:  50.060 | pTtrue:  40.245\n",
      "Jet: 690000 | pTraw: 64.664 | pTcorr:  8.742 | pTtrue:  14.848\n",
      "Jet: 700000 | pTraw: 143.266 | pTcorr:  86.316 | pTtrue:  77.210\n",
      "Jet: 710000 | pTraw: 72.064 | pTcorr:  17.224 | pTtrue:  15.344\n",
      "Jet: 720000 | pTraw: 72.701 | pTcorr:  21.176 | pTtrue:  27.714\n",
      "Jet: 730000 | pTraw: 54.516 | pTcorr:  5.850 | pTtrue:  11.211\n",
      "Jet: 740000 | pTraw: 75.861 | pTcorr:  24.360 | pTtrue:  33.535\n",
      "Jet: 750000 | pTraw: 157.103 | pTcorr:  91.455 | pTtrue:  77.945\n",
      "Jet: 760000 | pTraw: 116.482 | pTcorr:  55.500 | pTtrue:  54.427\n",
      "Jet: 770000 | pTraw: 5.315 | pTcorr:  0.823 | pTtrue:  11.521\n",
      "Jet: 780000 | pTraw: 100.946 | pTcorr:  54.448 | pTtrue:  53.524\n",
      "Jet: 790000 | pTraw: 67.303 | pTcorr:  16.377 | pTtrue:  12.083\n",
      "Jet: 800000 | pTraw: 75.960 | pTcorr:  23.686 | pTtrue:  16.567\n",
      "Jet: 810000 | pTraw: 89.681 | pTcorr:  50.907 | pTtrue:  46.109\n",
      "Jet: 820000 | pTraw: 136.452 | pTcorr:  84.111 | pTtrue:  86.219\n",
      "Jet: 830000 | pTraw: 91.678 | pTcorr:  33.445 | pTtrue:  47.332\n",
      "Jet: 840000 | pTraw: 127.121 | pTcorr:  71.445 | pTtrue:  79.870\n",
      "Jet: 850000 | pTraw: 113.264 | pTcorr:  71.791 | pTtrue:  64.244\n",
      "Jet: 860000 | pTraw: 61.575 | pTcorr:  6.621 | pTtrue:  17.868\n",
      "Jet: 870000 | pTraw: 53.474 | pTcorr:  5.804 | pTtrue:  10.052\n",
      "Jet: 880000 | pTraw: 61.978 | pTcorr:  12.894 | pTtrue:  20.750\n",
      "Jet: 890000 | pTraw: 79.422 | pTcorr:  18.837 | pTtrue:  25.596\n",
      "Jet: 900000 | pTraw: 71.826 | pTcorr:  18.290 | pTtrue:  14.530\n",
      "Jet: 910000 | pTraw: 103.111 | pTcorr:  42.548 | pTtrue:  59.365\n",
      "Backup .csv file closed.\n",
      "All data transferred to array. Testing with 915784 jets.\n",
      "\n",
      "Data set lengths: 915784 / 915784 / 915784\n",
      "Preparing to collect data from csv backup file...\n",
      "Jet: 10000 | pTraw: 76.772 | pTcorr:  25.412 | pTtrue:  12.343\n",
      "Jet: 20000 | pTraw: 76.825 | pTcorr:  18.858 | pTtrue:  14.997\n",
      "Jet: 30000 | pTraw: 130.271 | pTcorr:  69.983 | pTtrue:  62.754\n",
      "Jet: 40000 | pTraw: 83.504 | pTcorr:  46.873 | pTtrue:  42.615\n",
      "Jet: 50000 | pTraw: 113.017 | pTcorr:  67.883 | pTtrue:  59.847\n",
      "Jet: 60000 | pTraw: 77.088 | pTcorr:  18.947 | pTtrue:  17.512\n",
      "Jet: 70000 | pTraw: 61.772 | pTcorr:  15.456 | pTtrue:  11.802\n",
      "Jet: 80000 | pTraw: 66.055 | pTcorr:  20.790 | pTtrue:  20.498\n",
      "Jet: 90000 | pTraw: 90.830 | pTcorr:  22.508 | pTtrue:  13.438\n",
      "Jet: 100000 | pTraw: 117.814 | pTcorr:  62.559 | pTtrue:  58.738\n",
      "Jet: 110000 | pTraw: 107.363 | pTcorr:  64.200 | pTtrue:  53.033\n",
      "Jet: 120000 | pTraw: 85.843 | pTcorr:  53.148 | pTtrue:  64.545\n",
      "Jet: 130000 | pTraw: 133.463 | pTcorr:  77.272 | pTtrue:  87.516\n",
      "Jet: 140000 | pTraw: 62.021 | pTcorr:  27.308 | pTtrue:  22.707\n",
      "Jet: 150000 | pTraw: 115.308 | pTcorr:  64.298 | pTtrue:  55.040\n",
      "Jet: 160000 | pTraw: 41.271 | pTcorr:  11.644 | pTtrue:  19.017\n",
      "Jet: 170000 | pTraw: 55.888 | pTcorr:  10.802 | pTtrue:  18.747\n",
      "Jet: 180000 | pTraw: 65.394 | pTcorr:  18.887 | pTtrue:  11.186\n",
      "Jet: 190000 | pTraw: 86.784 | pTcorr:  40.350 | pTtrue:  32.000\n",
      "Jet: 200000 | pTraw: 68.908 | pTcorr:  19.054 | pTtrue:  23.058\n",
      "Jet: 210000 | pTraw: 95.070 | pTcorr:  45.530 | pTtrue:  43.073\n",
      "Jet: 220000 | pTraw: 5.527 | pTcorr: -1.501 | pTtrue:  16.173\n",
      "Jet: 230000 | pTraw: 71.667 | pTcorr:  16.284 | pTtrue:  19.537\n",
      "Jet: 240000 | pTraw: 77.438 | pTcorr:  9.736 | pTtrue:  22.153\n",
      "Jet: 250000 | pTraw: 67.984 | pTcorr:  30.710 | pTtrue:  29.098\n",
      "Jet: 260000 | pTraw: 62.522 | pTcorr:  10.492 | pTtrue:  27.303\n",
      "Jet: 270000 | pTraw: 70.189 | pTcorr:  11.918 | pTtrue:  11.825\n",
      "Jet: 280000 | pTraw: 139.422 | pTcorr:  83.168 | pTtrue:  82.147\n",
      "Jet: 290000 | pTraw: 81.677 | pTcorr:  30.350 | pTtrue:  12.514\n",
      "Jet: 300000 | pTraw: 87.778 | pTcorr:  34.120 | pTtrue:  30.530\n",
      "Jet: 310000 | pTraw: 81.503 | pTcorr:  26.189 | pTtrue:  33.609\n",
      "Jet: 320000 | pTraw: 96.909 | pTcorr:  42.392 | pTtrue:  24.273\n",
      "Jet: 330000 | pTraw: 86.129 | pTcorr:  37.876 | pTtrue:  31.180\n",
      "Jet: 340000 | pTraw: 75.750 | pTcorr:  21.051 | pTtrue:  15.789\n",
      "Jet: 350000 | pTraw: 5.029 | pTcorr: -3.712 | pTtrue:  20.749\n",
      "Jet: 360000 | pTraw: 67.199 | pTcorr:  10.632 | pTtrue:  11.716\n",
      "Jet: 370000 | pTraw: 101.544 | pTcorr:  56.713 | pTtrue:  67.971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet: 380000 | pTraw: 54.713 | pTcorr:  1.425 | pTtrue:  18.211\n",
      "Jet: 390000 | pTraw: 95.811 | pTcorr:  36.267 | pTtrue:  37.133\n",
      "Jet: 400000 | pTraw: 97.870 | pTcorr:  53.228 | pTtrue:  63.752\n",
      "Jet: 410000 | pTraw: 69.686 | pTcorr:  24.229 | pTtrue:  23.439\n",
      "Jet: 420000 | pTraw: 57.549 | pTcorr:  10.268 | pTtrue:  14.003\n",
      "Jet: 430000 | pTraw: 135.320 | pTcorr:  85.527 | pTtrue:  83.276\n",
      "Jet: 440000 | pTraw: 6.352 | pTcorr:  1.876 | pTtrue:  18.875\n",
      "Jet: 450000 | pTraw: 85.462 | pTcorr:  6.723 | pTtrue:  20.491\n",
      "Jet: 460000 | pTraw: 89.105 | pTcorr:  37.067 | pTtrue:  43.989\n",
      "Jet: 470000 | pTraw: 137.306 | pTcorr:  88.177 | pTtrue:  86.986\n",
      "Jet: 480000 | pTraw: 109.332 | pTcorr:  48.552 | pTtrue:  26.061\n",
      "Jet: 490000 | pTraw: 93.846 | pTcorr:  41.155 | pTtrue:  47.725\n",
      "Jet: 500000 | pTraw: 126.347 | pTcorr:  64.560 | pTtrue:  61.546\n",
      "Jet: 510000 | pTraw: 66.645 | pTcorr:  24.968 | pTtrue:  17.403\n",
      "Jet: 520000 | pTraw: 122.057 | pTcorr:  66.335 | pTtrue:  68.934\n",
      "Jet: 530000 | pTraw: 131.511 | pTcorr:  86.088 | pTtrue:  88.368\n",
      "Jet: 540000 | pTraw: 28.024 | pTcorr:  1.942 | pTtrue:  11.432\n",
      "Jet: 550000 | pTraw: 42.010 | pTcorr:  8.457 | pTtrue:  42.699\n",
      "Jet: 560000 | pTraw: 72.592 | pTcorr:  28.816 | pTtrue:  15.784\n",
      "Jet: 570000 | pTraw: 76.594 | pTcorr:  22.056 | pTtrue:  20.363\n",
      "Jet: 580000 | pTraw: 126.901 | pTcorr:  68.360 | pTtrue:  37.625\n",
      "Jet: 590000 | pTraw: 120.702 | pTcorr:  48.732 | pTtrue:  56.270\n",
      "Jet: 600000 | pTraw: 103.326 | pTcorr:  34.872 | pTtrue:  53.929\n",
      "Jet: 610000 | pTraw: 5.140 | pTcorr: -0.667 | pTtrue:  67.423\n",
      "Jet: 620000 | pTraw: 61.078 | pTcorr:  10.866 | pTtrue:  13.265\n",
      "Jet: 630000 | pTraw: 89.666 | pTcorr:  30.207 | pTtrue:  24.761\n",
      "Jet: 640000 | pTraw: 73.584 | pTcorr:  9.601 | pTtrue:  21.541\n",
      "Jet: 650000 | pTraw: 49.915 | pTcorr:  4.818 | pTtrue:  14.247\n",
      "Jet: 660000 | pTraw: 71.209 | pTcorr:  17.389 | pTtrue:  15.075\n",
      "Jet: 670000 | pTraw: 85.830 | pTcorr:  22.770 | pTtrue:  19.089\n",
      "Jet: 680000 | pTraw: 131.469 | pTcorr:  85.130 | pTtrue:  82.083\n",
      "Jet: 690000 | pTraw: 125.290 | pTcorr:  68.466 | pTtrue:  70.264\n",
      "Jet: 700000 | pTraw: 73.475 | pTcorr:  18.195 | pTtrue:  10.970\n",
      "Jet: 710000 | pTraw: 103.653 | pTcorr:  55.770 | pTtrue:  58.159\n",
      "Jet: 720000 | pTraw: 6.420 | pTcorr: -7.394 | pTtrue:  18.464\n",
      "Jet: 730000 | pTraw: 74.664 | pTcorr:  17.202 | pTtrue:  10.709\n",
      "Jet: 740000 | pTraw: 87.295 | pTcorr:  36.845 | pTtrue:  33.320\n",
      "Jet: 750000 | pTraw: 5.173 | pTcorr: -2.095 | pTtrue:  50.583\n",
      "Jet: 760000 | pTraw: 107.912 | pTcorr:  37.842 | pTtrue:  32.062\n",
      "Jet: 770000 | pTraw: 80.709 | pTcorr:  21.162 | pTtrue:  34.246\n",
      "Jet: 780000 | pTraw: 93.075 | pTcorr:  35.999 | pTtrue:  36.251\n",
      "Jet: 790000 | pTraw: 71.477 | pTcorr:  16.857 | pTtrue:  10.736\n",
      "Jet: 800000 | pTraw: 118.219 | pTcorr:  63.630 | pTtrue:  69.864\n",
      "Jet: 810000 | pTraw: 36.810 | pTcorr:  7.508 | pTtrue:  10.236\n",
      "Jet: 820000 | pTraw: 135.846 | pTcorr:  79.248 | pTtrue:  82.950\n",
      "Jet: 830000 | pTraw: 103.066 | pTcorr:  51.130 | pTtrue:  51.445\n",
      "Jet: 840000 | pTraw: 148.113 | pTcorr:  80.785 | pTtrue:  84.457\n",
      "Jet: 850000 | pTraw: 11.561 | pTcorr:  5.553 | pTtrue:  43.726\n",
      "Jet: 860000 | pTraw: 6.505 | pTcorr:  0.014 | pTtrue:  41.526\n",
      "Jet: 870000 | pTraw: 84.996 | pTcorr:  21.371 | pTtrue:  32.400\n",
      "Jet: 880000 | pTraw: 112.783 | pTcorr:  50.132 | pTtrue:  63.056\n",
      "Jet: 890000 | pTraw: 68.519 | pTcorr:  23.759 | pTtrue:  39.034\n",
      "Jet: 900000 | pTraw: 109.902 | pTcorr:  53.679 | pTtrue:  51.097\n",
      "Jet: 910000 | pTraw: 67.079 | pTcorr:  15.892 | pTtrue:  12.881\n",
      "Jet: 920000 | pTraw: 61.644 | pTcorr:  19.151 | pTtrue:  14.456\n",
      "Jet: 930000 | pTraw: 68.326 | pTcorr:  21.107 | pTtrue:  24.235\n",
      "Jet: 940000 | pTraw: 65.711 | pTcorr:  26.082 | pTtrue:  13.098\n",
      "Jet: 950000 | pTraw: 87.237 | pTcorr:  27.630 | pTtrue:  26.538\n",
      "Jet: 960000 | pTraw: 74.301 | pTcorr:  35.272 | pTtrue:  51.693\n",
      "Backup .csv file closed.\n",
      "All data transferred to array. Testing with 961986 jets.\n",
      "\n",
      "Data set lengths: 961986 / 961986 / 961986\n",
      "\n",
      "Ready! 2022/12/11 23:49:24\n"
     ]
    }
   ],
   "source": [
    "file_directory   = \"../Files/Comparison_Trial/Data/\"\n",
    "\n",
    "train_file_name  = \"ML_Prep_10_90_B8_Train.root\"\n",
    "train_tree_name  = \"Tree_10_90_B8_Train\"\n",
    "train_file_path  = file_directory + train_file_name\n",
    "train_csv_path   = file_directory + train_file_name[0:-5] + \"_Backup.csv\"\n",
    "train_range      = (10., 90.)\n",
    "\n",
    "test_file_name   = \"ML_Prep_10_90_B8_Test.root\"\n",
    "test_tree_name   = \"Tree_10_90_B8_Test\"\n",
    "test_file_path   = file_directory + test_file_name\n",
    "test_csv_path    = file_directory + test_file_name[0:-5] + \"_Backup.csv\"\n",
    "test_range       = (10., 90.)\n",
    "\n",
    "output_csv_name  = file_directory + \"ML_Results/Train_B8\"\n",
    "\n",
    "\n",
    "\n",
    "# Builds feature and target arrays from ROOT file\n",
    "# X_train, y_train, sc_train = Build_FeatureArrays_FromROOT(\n",
    "#     train_file_path, train_tree_name, train_csv_path, train_range[0], train_range[1])\n",
    "# X_test, y_test, sc_test = Build_FeatureArrays_FromROOT(\n",
    "#     test_file_path,  test_tree_name,  test_csv_path,  test_range[0],  test_range[1])\n",
    "\n",
    "# Rebuilds feature and target arrays from csv file (MUCH faster if csv has been made already)\n",
    "X_train, y_train, sc_train = Build_FeatureArrays_FromCSV(train_csv_path)\n",
    "X_test,  y_test,  sc_test  = Build_FeatureArrays_FromCSV(test_csv_path)\n",
    "\n",
    "\n",
    "\n",
    "# Set Features to train with\n",
    "# X_values[\n",
    "#    0  jet_pt_raw,      1  jet_pt_corr,     2  jet_mass,        3  jet_area, \n",
    "#    4  jet_area_err,    5  jet_const_n,     6  const_pt_mean,   7  const_pt_median, \n",
    "#    8  const_1_pt,      9  const_2_pt,      10 const_3_pt,      11 const_4_pt,\n",
    "#    12 const_5_pt,      13 const_6_pt,      14 const_7_pt,      15 const_8_pt,\n",
    "#    16 const_9_pt,      17 const_10_pt,     18 jet_y,           19 jet_phi,\n",
    "#    20 jet_rho]\n",
    "\n",
    "# Training with 1 feature\n",
    "feature_label_1feat = [\n",
    "    \"jet_pt_raw\"]\n",
    "feature_index_1feat = [0]\n",
    "\n",
    "# Training with 3 features\n",
    "feature_label_3feat = [\n",
    "    \"jet_pt_raw\", \"jet_area\", \"jet_rho\"]\n",
    "feature_index_3feat = [0, 3, 20]\n",
    "\n",
    "# Training with 12 features\n",
    "feature_label_12feat = [\n",
    "    \"jet_pt_raw\",    \"jet_pt_corr\",    \"jet_mass\",      \"jet_area\", \n",
    "    \"jet_const_n\",   \"const_pt_mean\",  \"const_1_pt\",    \"const_2_pt\",\n",
    "    \"const_3_pt\",    \"const_4_pt\",     \"jet_y\",         \"jet_rho\"]\n",
    "feature_index_12feat = [0, 1, 2, 3, 5, 6, 8, 9, 10, 11, 18, 20]\n",
    "\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print(\"\\nReady!\", dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4130c",
   "metadata": {},
   "source": [
    "## Training & Testing - 1, 3, and 12 Input Features\n",
    "1 Feature: pt_raw ONLY\n",
    "\n",
    "3 Features: pt_raw, jet_area, jet_rho\n",
    "\n",
    "12 Features: jet_pt_raw, jet_pt_corr, jet_mass, jet_area, jet_const_n, const_pt_mean, const_1_pt, const_2_pt, const_3_pt, const_4_pt, jet_y, jet_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe20d7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building training and testing selected feature arrays...\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 915784 \n",
      "\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 961986 \n",
      "\n",
      "\n",
      "Training linear regression estimator...\n",
      "\n",
      "----- Fitting Linear Regression Estimator -----\n",
      "\n",
      "\n",
      "Using StandardScaler. Data will be recentered and normalized.\n",
      "\n",
      "\n",
      "Linear Regression Fit:\n",
      " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Regression Coefficients:\n",
      "jet_pt_raw 18.287659172677568\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML weights .csv file closed.\n",
      "\n",
      "Testing 1 features on 18-22 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 28-32 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 38-42 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 48-52 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 58-62 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 68-72 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 1 features on 78-82 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Building training and testing selected feature arrays...\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 915784 \n",
      "\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 961986 \n",
      "\n",
      "\n",
      "Training linear regression estimator...\n",
      "\n",
      "----- Fitting Linear Regression Estimator -----\n",
      "\n",
      "\n",
      "Using StandardScaler. Data will be recentered and normalized.\n",
      "\n",
      "\n",
      "Linear Regression Fit:\n",
      " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Regression Coefficients:\n",
      "jet_pt_raw 25.60920699770461\n",
      "jet_area -11.766099102146164\n",
      "jet_rho -4.213189700853497\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML weights .csv file closed.\n",
      "\n",
      "Testing 3 features on 18-22 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 28-32 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 38-42 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 48-52 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 58-62 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 68-72 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 3 features on 78-82 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Building training and testing selected feature arrays...\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 915784 \n",
      "\n",
      "Selecting data from master array...\n",
      "Data ready. Feature array length: 961986 \n",
      "\n",
      "\n",
      "Training linear regression estimator...\n",
      "\n",
      "----- Fitting Linear Regression Estimator -----\n",
      "\n",
      "\n",
      "Using StandardScaler. Data will be recentered and normalized.\n",
      "\n",
      "\n",
      "Linear Regression Fit:\n",
      " Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearregression', LinearRegression())])\n",
      "Regression Coefficients:\n",
      "jet_pt_raw 26.7079559691915\n",
      "jet_pt_corr -1.9399244774118145\n",
      "jet_mass 6.416057238465879\n",
      "jet_area -6.271898609978765\n",
      "jet_const_n -12.417154827407174\n",
      "const_pt_mean 1.6714011096252326\n",
      "const_1_pt -0.3562960495713619\n",
      "const_2_pt -0.05490366661856144\n",
      "const_3_pt 0.1572957361970332\n",
      "const_4_pt -0.6988189982034938\n",
      "jet_y -0.0024062447296658985\n",
      "jet_rho -2.2184736461417267\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML weights .csv file closed.\n",
      "\n",
      "Testing 12 features on 18-22 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 28-32 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 38-42 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 48-52 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 58-62 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 68-72 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Testing 12 features on 78-82 GeV...\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "ML results .csv file closed.\n",
      "Test and save complete!\n",
      "\n",
      "\n",
      "Complete! 2022/12/11 23:50:19\n"
     ]
    }
   ],
   "source": [
    "test_min_max_array = [  # Array of min and max for pT ranges to test on\n",
    "    [18,22], [28,32], [38,42], [48,52], \n",
    "    [58,62], [68,72], [78,82]\n",
    "    ]\n",
    "feature_bundle = [\n",
    "    [feature_label_1feat,  feature_index_1feat], \n",
    "    [feature_label_3feat,  feature_index_3feat],\n",
    "    [feature_label_12feat, feature_index_12feat]\n",
    "    ]\n",
    "# train_bundle = [ # This may be implemented later to iterate through multiple training sets\n",
    "#     [X_train, y_train, sc_train]\n",
    "#     ]\n",
    "\n",
    "for feature_set in feature_bundle:\n",
    "    feature_label = feature_set[0]\n",
    "    feature_index = feature_set[1]\n",
    "    \n",
    "    output_csv_name_2 = output_csv_name + \"_F\" + str(len(feature_label)) + \"_\" + str(int(train_range[0])) + \"_\" + str(int(train_range[1]))\n",
    "    \n",
    "    # Builds training and testing arrays\n",
    "    print(\"\\nBuilding training and testing selected feature arrays...\")\n",
    "    X_train_select = Build_SelectFeatureArray(X_train, feature_index)\n",
    "    X_test_select  = Build_SelectFeatureArray(X_test, feature_index)\n",
    "\n",
    "    # Trains estimator\n",
    "    print(\"\\nTraining linear regression estimator...\")\n",
    "    lr_pipeline, lr_coeffs = Train_LinearRegression(\n",
    "        X_train_select, \n",
    "        y_train, \n",
    "        feature_label, \n",
    "        use_scaler = True)\n",
    "    print(type(lr_pipeline))\n",
    "    \n",
    "    Write_MLWeights_ToCSV(\n",
    "        output_csv_name_2 + \"_LR_Coeffs.csv\",\n",
    "        lr_coeffs, \n",
    "        feature_label\n",
    "        )\n",
    "\n",
    "    # Tests estimator and saves results\n",
    "\n",
    "    for min_max in test_min_max_array:\n",
    "        \n",
    "        output = \"\\nTesting \" + str(len(feature_index)) + \" features on \" + str(min_max[0]) + \"-\" + str(min_max[1]) + \" GeV...\"\n",
    "        print(output)\n",
    "        \n",
    "        csv_path = output_csv_name_2 + \"_Test_\" + str(int(min_max[0])) + \"_\" + str(int(min_max[1])) + \".csv\"\n",
    "        \n",
    "        TestAndSave_LinearRegression(\n",
    "            feature_label,\n",
    "            feature_index, \n",
    "            lr_pipeline, \n",
    "            lr_coeffs,\n",
    "            X_test_select,\n",
    "            y_test, \n",
    "            sc_test,\n",
    "            min_max[0],\n",
    "            min_max[1],   \n",
    "            csv_path,\n",
    "            use_scaler = True\n",
    "            )\n",
    "        \n",
    "        print(\"Test and save complete!\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print(\"\\nComplete!\", dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff5ff0",
   "metadata": {},
   "source": [
    "## OLD CODE\n",
    "#### WARNING: ALL CODE BELOW IS OUT OF DATE AND DOES NOT WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db848e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with individual Features\n",
    "\n",
    "# NOTE: This block tests with individual features (IF) by replacing the ML Estimators \n",
    "# after each feature is checked.\n",
    "\n",
    "X_ptRaw_train_A    = [ [X_train_A[i][0]] for i in range(len(X_train_A)) ]\n",
    "X_ptRaw_train_B    = [ [X_train_B[i][0]] for i in range(len(X_train_B)) ]\n",
    "X_ptCorr_train_A   = [ [X_train_A[i][1]] for i in range(len(X_train_A)) ]\n",
    "X_ptCorr_train_B   = [ [X_train_B[i][1]] for i in range(len(X_train_B)) ]\n",
    "#X_ptMean_train   = [ [X_values_A[i][6]] for i in range(len(X_values_A)) ]\n",
    "#X_ptMedian_train = [ [X_values_A[i][7]] for i in range(len(X_values_A)) ]\n",
    "#X_ptConst1_train = [ [X_values_A[i][8]] for i in range(len(X_values_A)) ]\n",
    "#X_ptConst2_train = [ [X_values_A[i][9]] for i in range(len(X_values_A)) ]\n",
    "#X_ptConst3_train = [ [X_values_A[i][10]] for i in range(len(X_values_A)) ]\n",
    "#X_ptConst4_train = [ [X_values_A[i][11]] for i in range(len(X_values_A)) ]\n",
    "\n",
    "lr_pipeline_ptRaw, rf_pipeline_ptRaw, nn_pipeline_ptRaw, features_arr_ptRaw = Train_ML_pt_Estimators(\n",
    "    X_ptRaw_train_A, y_train_A)\n",
    "lr_pipeline_ptCorr, rf_pipeline_ptCorr, nn_pipeline_ptCorr, features_arr_ptCorr = Train_ML_pt_Estimators(\n",
    "    X_ptCorr_train_A, y_train_A)\n",
    "\n",
    "for testing_tree_name in testing_tree_names :\n",
    "    output_base_name = testing_tree_name[5:]\n",
    "    pt_true_min = float(testing_tree_name[5:7])\n",
    "    pt_true_max = float(testing_tree_name[8:10])\n",
    "    print(pt_true_min, pt_true_max)\n",
    "    \n",
    "    X_test_A, y_test_A, sc_corr_test_arr_A, X_test_B, y_test_B, sc_corr_test_arr_B = Build_ML_Feature_Arrays(\n",
    "        input_file_path, testing_tree_name, pt_true_min, pt_true_max)\n",
    "    \n",
    "    X_ptRaw_A  = [ [X_test_A[i][0]] for i in range(len(X_test_A)) ]\n",
    "    X_ptCorr_A = [ [X_test_A[i][1]] for i in range(len(X_test_A)) ]\n",
    "    #X_ptMean   = [ [X_values_A[i][6]] for i in range(len(X_values_A)) ]\n",
    "    #X_ptMedian = [ [X_values_A[i][7]] for i in range(len(X_values_A)) ]\n",
    "    #X_ptConst1 = [ [X_values_A[i][8]] for i in range(len(X_values_A)) ]\n",
    "    #X_ptConst2 = [ [X_values_A[i][9]] for i in range(len(X_values_A)) ]\n",
    "    #X_ptConst3 = [ [X_values_A[i][10]] for i in range(len(X_values_A)) ]\n",
    "    #X_ptConst4 = [ [X_values_A[i][11]] for i in range(len(X_values_A)) ]\n",
    "\n",
    "    Test_ML_pt_Estimators(\n",
    "        X_ptRaw_A, \n",
    "        y_test_A, \n",
    "        sc_corr_test_arr_A,\n",
    "        lr_pipeline_ptRaw, rf_pipeline_ptRaw, nn_pipeline_ptRaw, features_arr_ptRaw,\n",
    "        output_base_name + \"_only_ptRaw_ptTrueA_compare\", \n",
    "        input_file_path, \n",
    "        40, -40., 40.,\n",
    "        False, # If true, outputs feature importance\n",
    "        True) # If true, compares to paper plots using 40, -40., 40., as limits\n",
    "\n",
    "    Test_ML_pt_Estimators(\n",
    "        X_ptCorr_A, \n",
    "        y_test_A, \n",
    "        sc_corr_test_arr_A,\n",
    "        lr_pipeline_ptCorr, rf_pipeline_ptCorr, nn_pipeline_ptCorr, features_arr_ptCorr,\n",
    "        output_base_name + \"_only_ptCorr_ptTrueA_compare\", \n",
    "        input_file_path,\n",
    "        40, -40., 40.,\n",
    "        False, # If true, outputs feature importance\n",
    "        True) # If true, compares to paper plots using 40, -40., 40., as limits\n",
    "    \n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67737eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a distribution plot of jets used for training\n",
    "\n",
    "output_file = ROOT.TFile.Open(output_file_path, \"UPDATE\")\n",
    "\n",
    "x_bin_min = 10.\n",
    "x_bin_max = 90.\n",
    "x_bin_count = (x_bin_max - x_bin_min)/2\n",
    "\n",
    "print(\"Output file opened and prepared.\")\n",
    "\n",
    "th1_ptTrue_distribution = ROOT.TH1D(\n",
    "    \"th1_\"+train_min+\"_\"+train_max+\"_Train_ptTrue_distribution\",\n",
    "    \"Jet p_{T}^{True} distribution for training with \"+train_min+\" to \"+train_max+\" GeV; p_{T}^{True} [GeV]; N_{ch jets}\",\n",
    "    (int(train_max) - int(train_min)) / 2, float(train_min), float(train_max))\n",
    "\n",
    "th1_ptTrue_distribution.SetDirectory(0)\n",
    "\n",
    "th1_ptTrue_distribution.Sumw2()\n",
    "\n",
    "print(\"Writing to output file.\")\n",
    "counter = 0;\n",
    "for ptTrue in y_train: \n",
    "    th1_ptTrue_distribution.Fill(ptTrue)\n",
    "    if (counter % 1000 == 0): print(\"Processed event:\", counter)\n",
    "    counter += 1\n",
    "    \n",
    "th1_ptTrue_distribution.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "\n",
    "output_file.Write()\n",
    "print(\"Output file written to.\")\n",
    "\n",
    "output_file.Close()\n",
    "print(\"Output file closed.\")\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print(\"Ready!\", dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb42c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    sc_values_arr,   # Array of simple correction values to compare against\n",
    "    lr_coeffs_arr,   # Feature coefficient values from Linear Regression\n",
    "    rf_features_arr  # Feature importance values from Random Forest\n",
    "    \n",
    "    sc_results = [sc_values_arr[i] for i in range(len(sc_values_arr))]\n",
    "    sc_results_delta = [sc_results[i] - y_test[i] for i in range(len(sc_results))]\n",
    "    \n",
    "    (file_prefix,\n",
    "    output_file_path,\n",
    "    x_bins, x_min, x_max,\n",
    "    bool_features,\n",
    "    bool_compare,\n",
    "    X_values)\n",
    "    \n",
    "    output_file = ROOT.TFile.Open(output_file_path, \"UPDATE\")\n",
    "    output_tree = ROOT.TTree(\"Tree_ML_\" + file_prefix[5:10], \"TTree of data from machine learning\")\n",
    "    \n",
    "    min_GeV = file_prefix[5:7]\n",
    "    max_GeV = file_prefix[8:10]\n",
    "    \n",
    "    # --- LINEAR REGRESSION ---\n",
    "    \n",
    "    print(\"\\n----- Testing Linear Regression Estimator -----\\n\")\n",
    "\n",
    "    # Outputs mean square error\n",
    "    output = np.mean((lr_pipeline.predict(X_test) - y_test)**2)\n",
    "    print(\"Mean Square Error:\\n\", output)\n",
    "\n",
    "    # Outputs the variance score\n",
    "    output = lr_pipeline.score(X_test, y_test)\n",
    "    print(\"Variance Score:\\n\", output)\n",
    "\n",
    "    # --- RANDOM FOREST REGRESSION ---\n",
    "    \n",
    "    print(\"\\n----- Testing Random Forest Regression Estimator -----\\n\")\n",
    "\n",
    "    # Outputs mean square error\n",
    "    output = np.mean((rf_pipeline.predict(X_test) - y_test)**2)\n",
    "    print(\"Mean Square Error:\\n\", output)\n",
    "\n",
    "    # Outputs the variance score\n",
    "    output = rf_pipeline.score(X_test, y_test)\n",
    "    print(\"Variance Score:\\n\", output)\n",
    "\n",
    "    # --- MULTILAYER PERCEPTRON REGRESSION ---\n",
    "    \n",
    "    print(\"\\n----- Testing Neural Network Regression Estimator -----\\n\")\n",
    "\n",
    "    # Outputs mean square error\n",
    "    output = np.mean((nn_pipeline.predict(X_test) - y_test)**2)\n",
    "    print(\"Mean Square Error:\\n\", output)\n",
    "\n",
    "    # Outputs the variance score\n",
    "    output = nn_pipeline.score(X_test, y_test)\n",
    "    print(\"Variance Score:\\n\", output)\n",
    "    \n",
    "    # --- GENERATE HISTOGRAMS ---\n",
    "    \n",
    "    th1d_data_feature_importance = ROOT.TH1D(\n",
    "        \"th1d_\" + file_prefix + \"_feature_importance\",\"\", len(features_arr), 0, 1)\n",
    "    \n",
    "    name_simple_correction = \"th1d_\" + file_prefix + \"_simple_correction\"\n",
    "    name_linear_regression = \"th1d_\" + file_prefix + \"_linear_regression\"\n",
    "    name_random_forest     = \"th1d_\" + file_prefix + \"_random_forest\"\n",
    "    name_neural_network    = \"th1d_\" + file_prefix + \"_neural_network\"\n",
    "    title_infix            = min_GeV + \" GeV to \" + max_GeV + \" GeV\"\n",
    "    \n",
    "    th1d_simple_correction = ROOT.TH1D(\n",
    "        name_simple_correction,\n",
    "        \"Jet p_{T} Delta for \" + title_infix + \", Background Subtraction; (p_{T, reco} - p_{T, true})/p_{T, true} [GeV]; N_{ch jets}\",\n",
    "        x_bins, x_min, x_max)\n",
    "    th1d_linear_regression = ROOT.TH1D(\n",
    "        name_linear_regression,\n",
    "        \"Jet p_{T} Delta for \" + title_infix + \", Linear Regression; (p_{T, reco} - p_{T, true})/p_{T, true} [GeV]; N_{ch jets}\",\n",
    "        x_bins, x_min, x_max)\n",
    "    th1d_random_forest = ROOT.TH1D(\n",
    "        name_random_forest,\n",
    "        \"Jet p_{T} Delta for \" + title_infix + \", Random Forest; (p_{T, reco} - p_{T, true})/p_{T, true} [GeV]; N_{ch jets}\",\n",
    "        x_bins, x_min, x_max)\n",
    "    th1d_neural_network = ROOT.TH1D(\n",
    "        name_neural_network,\n",
    "        \"Jet p_{T} Delta for \" + title_infix + \", Neural Network; (p_{T, reco} - p_{T, true})/p_{T, true} [GeV]; N_{ch jets}\",\n",
    "        x_bins, x_min, x_max)\n",
    "\n",
    "    th1d_simple_correction.SetDirectory(0)\n",
    "    th1d_linear_regression.SetDirectory(0)\n",
    "    th1d_random_forest.SetDirectory(0)\n",
    "    th1d_neural_network.SetDirectory(0)\n",
    "\n",
    "    th1d_simple_correction.Sumw2()\n",
    "    th1d_linear_regression.Sumw2()\n",
    "    th1d_random_forest.Sumw2()\n",
    "    th1d_neural_network.Sumw2()\n",
    "        \n",
    "    for i in range(len(features_arr)): th1d_data_feature_importance.SetBinContent(i+1, features_arr[i])\n",
    "    \n",
    "    lr_prediction_arr = lr_pipeline.predict(X_test)\n",
    "    rf_prediction_arr = rf_pipeline.predict(X_test)\n",
    "    nn_prediction_arr = nn_pipeline.predict(X_test)\n",
    "\n",
    "    output_file.cd()\n",
    "    \n",
    "    for i in range(len(X_test)) :\n",
    "        \n",
    "        # Normal\n",
    "        sc_correction = sc_correction_arr[i]\n",
    "        lr_prediction = lr_prediction_arr[i]\n",
    "        rf_prediction = rf_prediction_arr[i]\n",
    "        nn_prediction = nn_prediction_arr[i]\n",
    "\n",
    "        target = y_test[i]\n",
    "        \n",
    "        sc_delta = sc_correction - target\n",
    "        lr_delta = lr_prediction - target\n",
    "        rf_delta = rf_prediction - target\n",
    "        nn_delta = nn_prediction - target\n",
    "        \n",
    "        if bool_compare:\n",
    "            th1d_simple_correction.Fill( sc_delta )\n",
    "            th1d_linear_regression.Fill( lr_delta )\n",
    "            th1d_random_forest.Fill( rf_delta )\n",
    "            th1d_neural_network.Fill( nn_delta )\n",
    "        else:\n",
    "            if target != 0 :\n",
    "                th1d_simple_correction.Fill( sc_delta / target )\n",
    "                th1d_linear_regression.Fill( lr_delta / target )\n",
    "                th1d_random_forest.Fill( rf_delta / target )\n",
    "                th1d_neural_network.Fill( nn_delta / target )\n",
    "        \n",
    "        if i % 100 == 0 :\n",
    "            print(f\"Test {i:4.0f}: True: {y_test[i]:3.3f} , \", end=\"\")\n",
    "            print(f\"Pred(line): {lr_prediction:4.3f}({lr_delta: 4.3f}) , \", end=\"\")\n",
    "            print(f\"Pred(tree): {rf_prediction:4.3f}({rf_delta: 4.3f}) , \", end=\"\")\n",
    "            print(f\"Pred(perc): {nn_prediction:4.3f}({nn_delta: 4.3f})\")\n",
    "    \n",
    "    th1d_simple_correction.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "    th1d_linear_regression.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "    th1d_random_forest.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "    th1d_neural_network.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "    th1d_data_feature_importance.Write(\"\", ROOT.TObject.kOverwrite)\n",
    "    \n",
    "    output_file.Write()\n",
    "    print(\"Output file written to.\")\n",
    "\n",
    "    output_file.Close()\n",
    "    print(\"Output file closed.\")\n",
    "    \n",
    "    output_csv  = open((output_file_path[:-5] + \"_Train_\" + file_prefix[6:] + \".csv\"), 'w', newline='')\n",
    "    csv_writer  = csv.writer(output_csv)\n",
    "    csv_header = ['Jet Area', 'Jet pT Raw', 'Jet pT True', 'Jet pT Corrected', \n",
    "                  'Jet pT ML-LR', 'Jet pT ML-RF', 'Jet pT ML-NN']\n",
    "    \n",
    "    o_jet_area     = 0.\n",
    "    o_jet_pt_raw   = 0.\n",
    "    o_jet_pt_corr  = 0.\n",
    "    o_jet_pt_true  = 0.\n",
    "    o_jet_pt_ml_lr = 0.\n",
    "    o_jet_pt_ml_rf = 0.\n",
    "    o_jet_pt_ml_nn = 0.\n",
    "    \n",
    "    for i in range(len(X_test)) :\n",
    "        \n",
    "        # Normal\n",
    "        sc_correction = sc_correction_arr[i]\n",
    "        lr_prediction = lr_prediction_arr[i]\n",
    "        rf_prediction = rf_prediction_arr[i]\n",
    "        nn_prediction = nn_prediction_arr[i]\n",
    "\n",
    "        target = y_test[i]\n",
    "        \n",
    "        sc_delta = sc_correction - target\n",
    "        lr_delta = lr_prediction - target\n",
    "        rf_delta = rf_prediction - target\n",
    "        nn_delta = nn_prediction - target\n",
    "        \n",
    "        # Adds data to output CSV\n",
    "        o_jet_area     = X_values[i][3]\n",
    "        o_jet_pt_raw   = X_values[i][0]\n",
    "        o_jet_pt_true  = target\n",
    "        o_jet_pt_corr  = sc_correction\n",
    "        o_jet_pt_ml_lr = lr_prediction\n",
    "        o_jet_pt_ml_rf = rf_prediction\n",
    "        o_jet_pt_ml_nn = nn_prediction\n",
    "        \n",
    "        csv_row = [o_jet_area, o_jet_pt_raw, o_jet_pt_true, \n",
    "                   o_jet_pt_corr, o_jet_pt_ml_lr, o_jet_pt_ml_rf, o_jet_pt_ml_nn]\n",
    "        \n",
    "        csv_writer.writerow(csv_row)\n",
    "        \n",
    "        if i % 100 == 0 :\n",
    "            print(f\"Test {i:4.0f}: True: {y_test[i]:3.3f} , \", end=\"\")\n",
    "            print(f\"Pred(line): {lr_prediction:4.3f}({lr_delta: 4.3f}) , \", end=\"\")\n",
    "            print(f\"Pred(tree): {rf_prediction:4.3f}({rf_delta: 4.3f}) , \", end=\"\")\n",
    "            print(f\"Pred(perc): {nn_prediction:4.3f}({nn_delta: 4.3f})\")\n",
    "    \n",
    "    output_csv.close()\n",
    "    \n",
    "    print(\"Predictions and histogram filling complete.\")\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
